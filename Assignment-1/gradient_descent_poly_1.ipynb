{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e826d9e7-eb63-48fd-83ab-57f54e563ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.]\n",
      " [ 1.  2.  4.]\n",
      " [ 1.  3.  9.]\n",
      " [ 1.  4. 16.]\n",
      " [ 1.  5. 25.]]\n",
      "Final parameters: [ 1.00000000e+00  1.00000000e+00 -3.76001075e-13]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        # Hypothesis function\n",
    "        h = np.dot(X, theta)\n",
    "        \n",
    "        # Calculate the cost\n",
    "        cost = (1/(2*m)) * np.sum(np.square(h - y))\n",
    "        cost_history.append(cost)\n",
    "        # Update parameters using gradient descent\n",
    "        gradient = (1/m) * np.dot(X.T, (h - y))\n",
    "        theta -= alpha * gradient\n",
    "        # print(theta)\n",
    "\n",
    "    return theta, cost_history\n",
    "\n",
    "# Example data\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Feature matrix (including intercept term)\n",
    "y = np.array([2, 3, 4, 5, 6])  # Target vector\n",
    "\n",
    "degree = 2\n",
    "poly_features = PolynomialFeatures(degree=degree)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "print(X_poly)\n",
    "# Initial parameters\n",
    "theta = np.zeros(X_poly.shape[1])  # Initializing theta with zeros\n",
    "# Learning rate and number of iterations\n",
    "alpha = 0.001\n",
    "num_iters = 1000000\n",
    "\n",
    "# Performing gradient descent\n",
    "theta_final, cost_history = gradient_descent(X_poly, y, theta, alpha, num_iters)\n",
    "\n",
    "print(\"Final parameters:\", theta_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eb17a22-fcb1-4d96-904c-c00276463456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.]\n",
      " [ 1.  2.  4.]\n",
      " [ 1.  3.  9.]\n",
      " [ 1.  4. 16.]\n",
      " [ 1.  5. 25.]]\n",
      "Final parameters: [ 1.00000000e+00  1.00000000e+00 -1.87565241e-13]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, theta, mT, alpha, beta, num_iters):\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        # Hypothesis function\n",
    "        h = np.dot(X, theta)\n",
    "        gradient = (1/m) * np.dot(X.T, (h - y))\n",
    "        mT = beta * mT + (1 - beta) * gradient\n",
    "        \n",
    "        # Calculate the cost\n",
    "        cost = (1/(2*m)) * np.sum(np.square(h - y))\n",
    "        cost_history.append(cost)\n",
    "        # Update parameters using gradient descent\n",
    "        theta -= alpha * mT\n",
    "        # print(theta)\n",
    "\n",
    "    return theta, cost_history\n",
    "\n",
    "# Example data\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Feature matrix (including intercept term)\n",
    "y = np.array([2, 3, 4, 5, 6])  # Target vector\n",
    "\n",
    "degree = 2\n",
    "poly_features = PolynomialFeatures(degree=degree)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "print(X_poly)\n",
    "# Initial parameters\n",
    "theta = np.zeros(X_poly.shape[1])  # Initializing theta with zeros\n",
    "mT = np.zeros(X_poly.shape[1]) \n",
    "# Learning rate and number of iterations\n",
    "alpha = 0.001\n",
    "beta = 0.90\n",
    "num_iters = 5000000\n",
    "\n",
    "# Performing gradient descent\n",
    "theta_final, cost_history = gradient_descent(X_poly, y, theta, mT, alpha, beta, num_iters)\n",
    "\n",
    "print(\"Final parameters:\", theta_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f772c279-45ee-462c-85cc-2d5f061d75fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  1.]\n",
      " [ 1.  2.  4.]\n",
      " [ 1.  3.  9.]\n",
      " [ 1.  4. 16.]\n",
      " [ 1.  5. 25.]]\n",
      "Final parameters: [0.02163302 0.07203843 0.27923701]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, theta, mT, alpha, beta, num_iters):\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    \n",
    "    tolerance = 1 / (num_iters *100)\n",
    "    \n",
    "    for _ in range(num_iters):\n",
    "        # Hypothesis function\n",
    "        h = np.dot(X, theta)\n",
    "        \n",
    "        error_before = (1/(2*m)) * np.sum(np.square(h - y))\n",
    "        \n",
    "        gradient = (1/m) * np.dot(X.T, (h - y))\n",
    "        mT = beta * mT + (1 - beta) * gradient\n",
    "        \n",
    "        theta -= alpha * mT\n",
    "        # Calculate the cost\n",
    "        h = np.dot(X, theta)\n",
    "        \n",
    "        error_after = (1/(2*m)) * np.sum(np.square(h - y))\n",
    "\n",
    "        if error_after > error_before:\n",
    "            theta += alpha * mT\n",
    "            mT = (mT - (1 - beta) * gradient) / beta\n",
    "            alpha =  alpha / 2\n",
    "            continue\n",
    "        \n",
    "        # if error_before - error_after <= tolerance:\n",
    "        #     break\n",
    "        \n",
    "            \n",
    "        # cost_history.append(cost)\n",
    "        # Update parameters using gradient descent\n",
    "        # print(theta)\n",
    "\n",
    "    return theta, cost_history\n",
    "\n",
    "# Example data\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Feature matrix (including intercept term)\n",
    "y = np.array([2, 3, 4, 5, 6])  # Target vector\n",
    "\n",
    "degree = 2\n",
    "poly_features = PolynomialFeatures(degree=degree)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "print(X_poly)\n",
    "# Initial parameters\n",
    "theta = np.zeros(X_poly.shape[1])  # Initializing theta with zeros\n",
    "mT = np.zeros(X_poly.shape[1]) \n",
    "# Learning rate and number of iterations\n",
    "alpha = 0.001\n",
    "beta = 0.90\n",
    "num_iters = 500000\n",
    "\n",
    "# Performing gradient descent\n",
    "theta_final, cost_history = gradient_descent(X_poly, y, theta, mT, alpha, beta, num_iters)\n",
    "\n",
    "print(\"Final parameters:\", theta_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3279f083-0cba-4ff9-b6d3-b6ad8591a8eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
